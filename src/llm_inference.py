from typing import List

from vllm import LLM, SamplingParams
from vllm.sampling_params import GuidedDecodingParams

from .config import DEFAULT_SETTINGS, PROMPT_FORMATS
from .prompts import llm_judge_prompt


def inference_llm_judge_vllm(
    questions: List[str],
    model_answers: List[str],
    ground_truths: List[str],
    judge_model: str = DEFAULT_SETTINGS["judge_model"],
    max_model_length: int = DEFAULT_SETTINGS["judge_max_model_length"],
    positive_answer_option: str = "yes",
    negative_answer_option: str = "no",
) -> List[int]:
    """
    Uses an LLM to evaluate the correctness of model-generated answers.

    This function takes a list of questions, their corresponding model-generated answers,
    and ground-truth answers, and evaluates the correctness of each answer using
    a guided LLM-based judging approach. The model is instructed to output either
    a "yes" or "no" response, which is then interpreted as a boolean value.

    Args:
        questions (List[str]): A list of questions being answered.
        model_answers (List[str]): A list of answers generated by the model.
        ground_truths (List[str]): A list of correct answers for comparison.
        judge_model (str, optional): The model name used for judgment.
            Defaults to `DEFAULT_SETTINGS["judge_model"]`.
        max_model_length (int, optional): The maximum sequence length for the judge model.
            Defaults to `DEFAULT_SETTINGS["judge_max_model_length"]`.
        positive_answer_option (str, optional): The option for a positive answer.
        negative_answer_option (str, optional): The option for a negative answer.

    Returns:
        List[int]: A list of integer values where `1` indicates that the model's answer
        is correct according to the LLM judge, and `0` indicates an incorrect answer. -1 indicates
        an invalid answer or bad response.
    """
    answer_options = [positive_answer_option, negative_answer_option]

    assert len(questions) == len(model_answers) == len(ground_truths), (
        "List of questions, answers, and ground truths must be the same length"
    )

    guided_decoding_params = GuidedDecodingParams(choice=answer_options)
    sampling_params = SamplingParams(guided_decoding=guided_decoding_params)

    prompts = [
        llm_judge_prompt.format(
            question=question,
            answer=answer,
            ground_truth=ground_truth,
            pos_option=answer_options[0],
            neg_option=answer_options[1],
        )
        for question, answer, ground_truth in zip(questions, model_answers, ground_truths)
    ]
    prompts = apply_prompt_format(prompts, judge_model)

    llm = LLM(model=judge_model, max_model_len=max_model_length)

    outputs = llm.generate(
        prompts=prompts,
        sampling_params=sampling_params,
    )

    results = []
    for output in outputs:
        if output[0].outputs[0].text.lower() == positive_answer_option.lower():
            results.append(1)
        elif output[0].outputs[0].text.lower() == negative_answer_option.lower():
            results.append(0)
        else:
            # invalid answer
            results.append(-1)

    return results


def apply_prompt_format(prompts: List[str], model_name: str) -> List[str]:
    """
    Applies the appropriate prompt format based on the given model name.

    Args:
        prompts (List[str]): A list of input prompts to be formatted.
        model_name (str): The name of the model, which determines the prompt format.

    Raises:
        ValueError: If the model name does not match any predefined prompt formats.

    Returns:
        List[str]: A list of formatted prompts based on the specified model.
    """
    if "llama" in model_name.lower():
        return [PROMPT_FORMATS["llama3"]["prompt_template"].format(p) for p in prompts]
    elif "qwen" in model_name.lower():
        return [PROMPT_FORMATS["chatml"]["prompt_template"].format(p) for p in prompts]
    else:
        raise ValueError(f"No prompt template found in config.py for {model_name}")
