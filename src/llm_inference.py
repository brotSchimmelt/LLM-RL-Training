from typing import List

from vllm import LLM, SamplingParams
from vllm.sampling_params import GuidedDecodingParams

from config import DEFAULT_SETTINGS
from prompts import llm_judge_prompt


def inference_llm_judge_vllm(
    questions: List[str],
    model_answers: List[str],
    ground_truths: List[str],
    judge_model: str = DEFAULT_SETTINGS["judge_model"],
    max_model_length: int = DEFAULT_SETTINGS["judge_max_model_length"],
    positive_answer_option: str = "yes",
    negative_answer_option: str = "no",
) -> List[bool]:
    """
    Uses an LLM to evaluate the correctness of model-generated answers.

    This function takes a list of questions, their corresponding model-generated answers,
    and ground-truth answers, and evaluates the correctness of each answer using
    a guided LLM-based judging approach. The model is instructed to output either
    a "yes" or "no" response, which is then interpreted as a boolean value.

    Args:
        questions (List[str]): A list of questions being answered.
        model_answers (List[str]): A list of answers generated by the model.
        ground_truths (List[str]): A list of correct answers for comparison.
        judge_model (str, optional): The model name used for judgment.
            Defaults to `DEFAULT_SETTINGS["judge_model"]`.
        max_model_length (int, optional): The maximum sequence length for the judge model.
            Defaults to `DEFAULT_SETTINGS["judge_max_model_length"]`.
        positive_answer_option (str, optional): The option for a positive answer.
        negative_answer_option (str, optional): The option for a negative answer.

    Returns:
        List[bool]: A list of boolean values where `True` indicates that the model's answer
        is correct according to the LLM judge, and `False` indicates an incorrect answer.
    """
    answer_options = [positive_answer_option, negative_answer_option]

    assert len(questions) == len(model_answers) == len(ground_truths), (
        "List of questions, answers, and ground truths must be the same length"
    )

    guided_decoding_params = GuidedDecodingParams(choice=answer_options)
    sampling_params = SamplingParams(guided_decoding=guided_decoding_params)

    prompts = [
        llm_judge_prompt.format(
            question=question,
            answer=answer,
            ground_truth=ground_truth,
            pos_option=answer_options[0],
            neg_option=answer_options[1],
        )
        for question, answer, ground_truth in zip(questions, model_answers, ground_truths)
    ]

    llm = LLM(model=judge_model, max_model_len=max_model_length)

    outputs = llm.generate(
        prompts=prompts,
        sampling_params=sampling_params,
    )

    results = []
    for output in outputs:
        if output[0].outputs[0].text.lower() == answer_options[0].lower():
            results.append(True)
        else:
            results.append(False)

    return results
